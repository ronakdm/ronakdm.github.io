---
layout: default
title: 'Projects'
---

<h1 style="margin-top:0;">Projects</h1>

<p class="lead"><small>
        In the following, "Notes and Tutorials" constains mathematical expositions of various topics that Iâ€™ve found
        interesting, relevant, or difficult
        to understand, while "Applied Projects" contains implementations of methods to solve an empirical
        problem.</small></p>

<h4>Notes and Tutorials:</h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <a href="projects/Wake_Sleep_Review.pdf">A Brief History of the Wake
            Sleep Algorithm</a>. </li>
    <li style="padding:0px;margin:0px 20px 0px;"> <a href="projects/An_Alternative_Exposition_of_PCA.pdf">An Alternative
            Exposition of PCA</a>. </li>
    <br />
</p>

<h4>Applied Projects:</h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
<ul>
    <li style="padding:0px;margin:0px 0px 0px;"> <i>Designing a Tetric Controller with Cross Entropy Method (CEM)
            Optimization</i>. Joint work with <a href="https://github.com/cailinw">Cailin Winston</a> and <a
            href="https://github.com/ptrmcl">Peter Michael</a>.
        <br />
        <b>Abstract</b>: Designing a high-performing Tetris player is a fundamental benchmark problem in artificial
        intelligence (AI), due to the the difficulty of the problem. First and foremost, as in all reinforcement
        learning settings, actions taken by the agent affect the environment. The state space is large and
        discrete, and the reward signal does not lend itself to typical gradient-based optimization. Computationally,
        success in Tetris is inherently tied to executing a long game, which implies a training
        time that grows with performance. In this project, we collect hand-designed features from the
        literature to handle the unruliness of the state space, and use a variant of Cross Entropy Method
        optimization for our non-differentiable reward function. We adjust the method to combat some of
        the statistical and computational pitfalls that agents run into when using this algorithm. In doing
        so, we achieve a strong-performing Tetris player with a relatively simple parametrization.
        <ul>
            <li> <a href="https://github.com/ronakdm/tetris_cem">Project repo</a>. </li>
            <li> <a href="projects/tetris_cem.pdf">Project write-up</a>.
        </ul>
    </li>
    <li style="padding:0px;margin:0px 0px 0px;"> <i>Language Modeling through Inverse Reinforcement Learning</i>. Joint
        work with <a href="https://github.com/cailinw">Cailin Winston</a> and <a href="https://github.com/ptrmcl">Peter
            Michael</a>.
        <ul>
            <li> <a href="https://github.com/ronakdm/irl-text-generation">Project repo</a>. </li>
            <li> <a href="projects/irl_text_generation.pdf">Project write-up</a>.
        </ul>
    </li>
</ul>
<br />
</p>